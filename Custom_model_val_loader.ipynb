{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom model val loader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12CGDgxr0MbIzgh_dPnI3dfMiiVYt_b-b",
      "authorship_tag": "ABX9TyNaL7dO22qn+KtuabLyT1m5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xyzkpsf/CS-W182-CV-Project/blob/main/Custom_model_val_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdTraSe-fLfi"
      },
      "source": [
        "try:\n",
        "    import torchbearer\n",
        "except:\n",
        "    !pip install -q torchbearer\n",
        "    import torchbearer\n",
        "\n",
        "!pip install livelossplot\n",
        "print(torchbearer.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W76R8DyFgAfN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision.io import read_image\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from livelossplot import PlotLosses\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQxbsEQRgHGj"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/Spring 2021/CS 182/tiny-imagenet-200.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ4bQx9MgLWP"
      },
      "source": [
        "class CustomNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomNet, self).__init__()\n",
        "        # for layer 0\n",
        "        self.conv = nn.Conv2d(3, 32, kernel_size=3, stride=1,\n",
        "                              padding=1, bias=False)\n",
        "  \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        # for layer 1\n",
        "        self.conv_1 = nn.Conv2d(32, 128, kernel_size=3, stride=1, \n",
        "                                padding=1, bias=False)\n",
        "        # for layer 2 - 4\n",
        "        self.conv_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, \n",
        "                                padding=1, bias=False)\n",
        "        \n",
        "        # for layer 6\n",
        "        self.conv_3 = nn.Conv2d(160, 256, kernel_size=3, stride=1, \n",
        "                                padding=1, bias=False)\n",
        "        # for layer 7 - 9\n",
        "        self.conv_4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, \n",
        "                                padding=1, bias=False)\n",
        "        # for layer 11\n",
        "        self.conv_5 = nn.Conv2d(416, 512, kernel_size=3, stride=1, \n",
        "                                padding=1, bias=False)\n",
        "        # for layer 12 - 14\n",
        "        self.conv_6 = nn.Conv2d(512, 512, kernel_size=3, stride=1, \n",
        "                                padding=1, bias=False)\n",
        "        \n",
        "        # for layer 16\n",
        "        self.conv_7 = nn.Conv2d(928, 200, kernel_size=1, stride=1, \n",
        "                                padding=1, bias=False)\n",
        "        \n",
        "        self.bn_32 = nn.BatchNorm2d(32)\n",
        "        self.bn_128 = nn.BatchNorm2d(128)\n",
        "        self.bn_160 = nn.BatchNorm2d(160)\n",
        "        self.bn_256 = nn.BatchNorm2d(256)\n",
        "        self.bn_416 = nn.BatchNorm2d(416)\n",
        "        self.bn_512 = nn.BatchNorm2d(512)\n",
        "        self.bn_928 = nn.BatchNorm2d(928)\n",
        "\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "        self.avg_pool = nn.AvgPool2d(8)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # block 1, layer 0\n",
        "        layer0 = self.conv(x)\n",
        "        layer0 = self.bn_32(layer0)\n",
        "        layer0 = self.relu(layer0)\n",
        "        skip_connection_1 = layer0\n",
        "        \n",
        "        # block 2, layer 1 - 5\n",
        "        # layer 1\n",
        "        layer1 = self.conv_1(layer0)\n",
        "        layer1 = self.bn_128(layer1)\n",
        "        layer1 = self.relu(layer1)\n",
        "        # layer 2 - 4\n",
        "        out = layer1\n",
        "        for _ in range(3):\n",
        "            out = self.conv_2(out)\n",
        "            out = self.bn_128(out)\n",
        "            out = self.relu(out)\n",
        "        layer4 = out\n",
        "        # layer 5\n",
        "        layer5 = torch.cat((skip_connection_1, layer4), 1)\n",
        "        layer5 = self.bn_160(layer5)\n",
        "        layer5 = self.relu(layer5)\n",
        "        layer5 = self.max_pool(layer5)\n",
        "        skip_connection_2 = layer5\n",
        "        \n",
        "        # block 3, layer 6 - 10\n",
        "        # layer 6\n",
        "        layer6 = self.conv_3(layer5)\n",
        "        layer6 = self.bn_256(layer6)\n",
        "        layer6 = self.relu(layer6)\n",
        "        # layer 7 - 9\n",
        "        out = layer6\n",
        "        for _ in range(3):\n",
        "            out = self.conv_4(out)\n",
        "            out = self.bn_256(out)\n",
        "            out = self.relu(out)\n",
        "        layer9 = out\n",
        "        # layer 10\n",
        "        layer10 = torch.cat((skip_connection_2, layer9), 1)\n",
        "        layer10 = self.bn_416(layer10)\n",
        "        layer10 = self.relu(layer10)\n",
        "        layer10 = self.max_pool(layer10)\n",
        "        skip_connection_3 = layer10\n",
        "        \n",
        "        # block 4, layer 11 - 15\n",
        "        # layer 11\n",
        "        layer11 = self.conv_5(layer10)\n",
        "        layer11 = self.bn_512(layer11)\n",
        "        layer11 = self.relu(layer11)\n",
        "        # layer 12 - 14\n",
        "        out = layer11\n",
        "        for _ in range(3):\n",
        "            out = self.conv_6(out)\n",
        "            out = self.bn_512(out)\n",
        "            out = self.relu(out)\n",
        "        layer14 = out\n",
        "        # layer 15\n",
        "        layer15 = torch.cat((skip_connection_3, layer14), 1)\n",
        "        layer15 = self.bn_928(layer15)\n",
        "        layer15 = self.relu(layer15)\n",
        "        layer15 = self.max_pool(layer15)\n",
        "        \n",
        "        # layer 16 and output\n",
        "        layer16 = self.conv_7(layer15)\n",
        "        out = self.avg_pool(layer16)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnJkIyEvJFzg"
      },
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, val_label_dir, img_dir, train_path, transform=None, target_transform=None):\n",
        "        super(CustomImageDataset, self).__init__()\n",
        "        self.val_label_file = pd.read_csv(val_label_dir, delimiter = \"\\t\", names=[\"pics\", \"labels\", \"_1\", \"_2\", \"_3\", \"_4\"])\n",
        "        self.img_labels = self.val_label_file[[\"pics\", 'labels']]\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.classes, self.class_to_idx = self._find_classes(train_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def _find_classes(self, dir):\n",
        "        \"\"\"\n",
        "        Finds the class folders in a dataset.\n",
        "        Args:\n",
        "            dir (string): Root directory path.\n",
        "        Returns:\n",
        "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
        "        Ensures:\n",
        "            No class is a subdirectory of another.\n",
        "        \"\"\"\n",
        "        if sys.version_info >= (3, 5):\n",
        "            # Faster and available in Python 3.5 and above\n",
        "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
        "        else:\n",
        "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "        classes.sort()\n",
        "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "        return classes, class_to_idx\n",
        "\n",
        "    def pad(self, img):\n",
        "        padding = np.ones((64, 64, 2))\n",
        "        img = img.reshape((64, 64, 1))\n",
        "        img = np.concatenate((img, padding), axis=2)\n",
        "        return img\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        img = Image.open(img_path)\n",
        "        img = copy.deepcopy(np.asarray(img))\n",
        "        # if it has less than 3 channels\n",
        "        if img.shape != (64, 64, 3):\n",
        "            img = self.pad(img)\n",
        "        #print(img.shape)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        label = self.class_to_idx[label]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        #sample = {\"image\": img, \"label\": label}\n",
        "        return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12uDWdL4gNlD"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/tiny-imagenet-200'\n",
        "\n",
        "val_label_dir = '/content/tiny-imagenet-200/val/val_annotations.txt'\n",
        "\n",
        "image_datasets = {}\n",
        "\n",
        "image_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train'),\n",
        "                                          data_transforms['train'])\n",
        "\n",
        "image_datasets['val'] = CustomImageDataset(val_label_dir, data_dir+'/val/images', \n",
        "                                           os.path.join(data_dir, 'train'),\n",
        "                                           transform=data_transforms['val'])\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=128,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "                                            for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qespvjx8QCPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c224d6b3-e8ae-4e74-f726-c3511604edaa"
      },
      "source": [
        "x, y = next(iter(image_datasets['val']))\n",
        "print(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.1490, 0.7216, 0.7961,  ..., 0.6039, 0.5451, 0.5216],\n",
            "         [0.0314, 0.0275, 0.2392,  ..., 0.5098, 0.4627, 0.4745],\n",
            "         [0.7490, 0.8078, 0.4000,  ..., 0.5059, 0.3765, 0.4235],\n",
            "         ...,\n",
            "         [0.2078, 0.2196, 0.2588,  ..., 0.6980, 0.7176, 0.6941],\n",
            "         [0.1647, 0.1216, 0.1137,  ..., 0.7059, 0.7333, 0.7216],\n",
            "         [0.6196, 0.5882, 0.5961,  ..., 0.6784, 0.7255, 0.7216]],\n",
            "\n",
            "        [[0.1922, 0.7098, 0.7020,  ..., 0.6196, 0.5647, 0.5412],\n",
            "         [0.1176, 0.0588, 0.1765,  ..., 0.5255, 0.4784, 0.4941],\n",
            "         [0.8941, 0.8863, 0.3882,  ..., 0.5216, 0.4039, 0.4510],\n",
            "         ...,\n",
            "         [0.2078, 0.2196, 0.2588,  ..., 0.6941, 0.7137, 0.6902],\n",
            "         [0.1608, 0.1176, 0.1098,  ..., 0.7020, 0.7294, 0.7176],\n",
            "         [0.6157, 0.5843, 0.5922,  ..., 0.6745, 0.7216, 0.7176]],\n",
            "\n",
            "        [[0.2078, 0.7451, 0.7647,  ..., 0.5529, 0.4863, 0.4549],\n",
            "         [0.1098, 0.0706, 0.2196,  ..., 0.4588, 0.4118, 0.4157],\n",
            "         [0.8667, 0.8824, 0.4157,  ..., 0.4627, 0.3412, 0.3804],\n",
            "         ...,\n",
            "         [0.2000, 0.2118, 0.2510,  ..., 0.6784, 0.6980, 0.6745],\n",
            "         [0.1451, 0.1020, 0.0941,  ..., 0.6863, 0.7137, 0.7020],\n",
            "         [0.6000, 0.5686, 0.5765,  ..., 0.6588, 0.7059, 0.7020]]]) 107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llxIgvXAgi2a"
      },
      "source": [
        "# No pretrained parameters\n",
        "model = CustomNet()\n",
        "model = model.to(device)\n",
        "\n",
        "#Multi GPU\n",
        "#model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
        "\n",
        "#Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EzHIHuwgtHI"
      },
      "source": [
        "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=30):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    since = time.time()\n",
        "    liveloss = PlotLosses()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for i,(inputs, labels) in enumerate(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                print(\"\\rIteration: {}/{}, Loss: {}.\".format(i+1, len(dataloaders[phase]), loss.item() * inputs.size(0)), end=\"\")\n",
        "                sys.stdout.flush()\n",
        "                \n",
        "                \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            if phase == 'train':\n",
        "                avg_loss = epoch_loss\n",
        "                t_acc = epoch_acc\n",
        "            else:\n",
        "                val_loss = epoch_loss\n",
        "                val_acc = epoch_acc\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                \n",
        "        liveloss.update({\n",
        "            'log loss': avg_loss,\n",
        "            'val_log loss': val_loss,\n",
        "            'accuracy': t_acc,\n",
        "            'val_accuracy': val_acc\n",
        "        })\n",
        "                \n",
        "        liveloss.draw()\n",
        "        print('Train Loss: {:.4f} Acc: {:.4f}'.format(avg_loss, t_acc))\n",
        "        print(  'Val Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc))\n",
        "        print('Best Val Accuracy: {}'.format(best_acc))\n",
        "        print()\n",
        "    \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "GEmHKrGqhGGJ",
        "outputId": "9325dce9-2905-4f48-de7f-39f430dfc841"
      },
      "source": [
        "model = train_model(model, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 782/782, Loss: 136.7656707763672."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:114: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:114: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-53d898e6c70e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = train_model(model, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=50)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-112-d0a363e4ff52>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcN3IbaPhJeT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}